{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks Weight Initialisation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLKOfvdI9/f/TZ3EeYharu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piyushmittal2192/neural_networks/blob/main/Neural_Networks_Weight_Initialisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUWzIDFDDgL8"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.randn(m) - returns m observations from a standard normal distribution of mean:0 and std:1\n",
        "# torch.randn(n,m) - returns n*m matrix of observations from a standard normal distribution of mean:0 and std:1 \n",
        "x = torch.randn(512)\n",
        "# x"
      ],
      "metadata": {
        "id": "ujiVMxBsD0BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### lets say we have initial input x of 512 features\n",
        "#### we have initial weight(W) initialization of 512,512 dimension\n",
        "#### we have 100 neural layers and no activation function\n",
        "#### for each layer we apply matrix multiplication of W and x\n",
        "#### since we have 512,512 . 512,1 ouptut would also be 512,1\n",
        "#### if we had 256,512 . 512,1 ouptut would also be 256,1 : then for each subsequent layer we would have to initialize the weights accordingly\n",
        "\n",
        "#### when we run the code for larger number of layers in this type of weight initialization we see gradients keeps exploding as we can see the output from last layers is very big\n",
        "# and if we increase the number of layers or dimesion of the features or weight initialization matrix the output would go to infinity i.e. we will see exploding problem\n",
        "x = torch.randn(20)\n",
        "W = torch.randn(20,20)\n",
        "for i in range(200):\n",
        "  x = W @ x # @ is same as np.matmul()\n",
        "  if torch.isnan(x.std()):\n",
        "    print(\"Reached exlploding problem at {} layer\".format(i))\n",
        "    break\n",
        "\n",
        "x.mean() , x.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Ek3_aNEWjJ",
        "outputId": "ec122b47-75ca-4455-e0af-635e373f820e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached exlploding problem at 53 layer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(nan), tensor(nan))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### lets say we have initial input x of 512 features\n",
        "#### we have initial weight(W) initialization of 512,512 dimension with very small values\n",
        "#### we have 100 neural layers and no activation function\n",
        "#### for each layer we apply matrix multiplication of W and x\n",
        "#### since we have 512,512 . 512,1 ouptut would also be 512,1\n",
        "#### if we had 256,512 . 512,1 ouptut would also be 256,1 : then for each subsequent layer we would have to initialize the weights accordingly\n",
        "\n",
        "#### when we run the code for larger number of layers in this type of weight initialization we see gradients keeps diminishing as we can see the output from last layers is very small\n",
        "# and if we increase the number of layers or dimesion of the features or weight initialization matrix the output would get very small i.e. we will see vanisihing problem\n",
        "x = torch.randn(20)\n",
        "W = torch.randn(20,20) * 0.01\n",
        "for i in range(200):\n",
        "  x = W @ x # @ is same as np.matmul()\n",
        "  if x.std() == 0:\n",
        "    print(\"Reached vanishing problem at {} layer\".format(i))\n",
        "    break\n",
        "\n",
        "x.mean() , x.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGBxaV-6H8sC",
        "outputId": "1605da43-47d9-40de-b4af-35646c101532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached vanishing problem at 33 layer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.), tensor(0.))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqMqKiJDEmqd",
        "outputId": "f34da9a6-6b41-4b9b-acff-653537eee798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.7870,  0.7699])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp9Si6KNE3xs",
        "outputId": "0bf4dc62-1e92-4746-9bd8-1a467545aa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5025,  0.6332],\n",
              "        [ 1.0166, -0.1459]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-0.7870*0.5025 + 0.7699*0.6332"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi5t83qME4GV",
        "outputId": "77478ac3-36b3-41ae-e068-c3ce37e9e749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09203318000000005"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ElszU0XwFAmE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}